{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade accelerate\n",
    "!pip uninstall -y transformers accelerate\n",
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ntlk (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for ntlk\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install  ntlk \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import ntlk\n",
    "from ntlk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "ntlk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cpkt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_cpkt)\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_cpkt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n",
    "!unzip summarizer-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum = load_from_disk('samsum_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lengths = [len(dataset_samsum[split] ) for split in dataset_samsum]\n",
    "print(f\"Split length: {split_lengths} \" )\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDiaglogue\")\n",
    "\n",
    "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
    "print(\"\\nSummary\")\n",
    "print(dataset_samsum[\"test\"][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(example_batch):\n",
    "    input_encoding = tokenizer(example_batch['dialogue'], max_length=512, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length=512, truncation=True)\n",
    "    return {\n",
    "        'input_ids': input_encoding['input_ids'],\n",
    "        'attention_mask': input_encoding['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum_pt = dataset_samsum.map(convert_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model= model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_gpu_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps = 10, \n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model= model_pegasus, args=trainer_args, \n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"test\"], \n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalution\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Split the dataset into smaller batches that we can process \n",
    "    Yield Successive batch-sized chunks from list_of_elements \"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i:i+batch_size]\n",
    "\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer, batch_size =16, device = device,\n",
    "                                column_text=\"article\", column_summary=\"highlights\"):\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size ))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "        inputs = tokenizer(article_batch, max_length = 512, truncation=True, padding =\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries  = model.generate(input_ids= inputs[\"input_ids\"].to(device),\n",
    "                                    attention_mask = inputs[\"attention_mask\"]. to(device),\n",
    "                                    lenght_penalty=0.8, num_beans=8, max_length=128)\n",
    "        \n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokeniztion_spaces=True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"\" , \"\") for d in decoded_summaries]\n",
    "        metric.add_batch(prediction=decoded_summaries, references = target_batch)\n",
    "    \n",
    "    #Finally compute and return the ROUGE Scores\n",
    "    \n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = calculate_metric_on_test_ds(\n",
    "    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size =2, \n",
    "    column_text='dialogue', column_summary='summary'\n",
    ")\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[f'pegausus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save the model\n",
    "model_pegasus.svae_pretrained(\"pegasus-samsum-model\")\n",
    "##Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenzier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenzier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "gen_kwargs = {\"lenght_penalty\":0.8, \"num_beams\":8, \"max_length\": 128}\n",
    "\n",
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "pipe = pipeline(\"summarization\", model =\"pegasus-samsum-model\", tokenizer=tokenizer)\n",
    "\n",
    "##\n",
    "print(\"Dialogue\")\n",
    "print(sample_text)\n",
    "\n",
    "print(\"\\nReferences\")\n",
    "print(reference)\n",
    "print(\"\\nModel Summary\")\n",
    "print(pipe(sample_text,**gen_kwargs)[0][\"summary_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
